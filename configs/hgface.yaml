model:
  name: "Llama-2-7b-hf"
  base_model: "meta-llama/Llama-2-7b-hf"
  type: "hgface"
  tokenizer: "meta-llama/Llama-2-7b-hf"
  ckpt_dir: "CKPT_DIR"
  dir: "MODEL_DIR"
  config:
    max_seq_len: 2048
  lora:
    r: 16
    lora_alpha: 32
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
    dropout: 0.1
    bias: "none"
    lora_dropout: 0.1
    modules_to_save:
      - "classifier"
    

training:
  batch_size: 4
  test_batch_size: 16
  epochs: 3000
  learning_rate: 5e-5
  step_size: 20
  gamma: 0.7
  patience: 3000
  min_delta: 0.0
  weight_decay: 1e-4
  betas: [0.9, 0.999]
  optimizer: "adamw"
  loss_function: "cross_entropy"

data:
  short_answer: True
  easy_task: True
  hint: True
  max_a_length: 16

lctm:
  uq_path: "CSV_PATH"
